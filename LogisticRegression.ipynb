{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class softmax_regression():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.W = None\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def softmax(arr):\n",
    "        \"\"\"\n",
    "        Rowwise computation of softmax.\n",
    "        Input:\n",
    "         - arr: an array of dimension n x k (in this context X @ W.T)\n",
    "        Output:\n",
    "         - an array of the same dimension as input\n",
    "        \"\"\"\n",
    "        a = arr.max(axis=1)\n",
    "        ex_arr = np.exp(arr - a[:,None])\n",
    "        return ex_arr / ex_arr.sum(axis=1)[:,None]\n",
    "    \n",
    "    @staticmethod\n",
    "    def log_sum_exp(arr):\n",
    "        \"\"\"\n",
    "        Rowwise computation of log-sum-exp.\n",
    "        Input: \n",
    "         - arr: an array of dimension n x k (in this context X @ W.T)\n",
    "        Output:\n",
    "         - an array of dimension n x 1\n",
    "        \"\"\"\n",
    "        a = arr.max(axis=1)\n",
    "        return a + np.log(np.sum(np.exp(arr - a[:,None]), axis=1))\n",
    "    \n",
    "    @staticmethod\n",
    "    def one_hot(vec):\n",
    "        \"\"\"\n",
    "        One hot encoder for response:\n",
    "        Input:\n",
    "         - vec: an n x 1 (response) vector \n",
    "        Output:\n",
    "         - an n x k one hot encoding of the input\n",
    "        \"\"\"\n",
    "        # one hot encoding from: \n",
    "        #  - https://stackoverflow.com/questions/29831489/numpy-1-hot-array\n",
    "        one_hot = np.zeros((len(vec), len(np.unique(vec))))\n",
    "        one_hot[np.arange(len(vec)), vec] = 1\n",
    "        return one_hot\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient_descent(f, grad_f, w0, alpha=0.005, eps=0.01):\n",
    "        \"\"\"\n",
    "        Gradient Descent for fitting:\n",
    "        Input: \n",
    "         - f:      the function to be minimized (loss in this context)\n",
    "         - grad_f: the gradient of the function to be minimized\n",
    "         - w0:     an initial guess\n",
    "         - alpha:  the learning rate (constant at this point)\n",
    "         - eps:    the tolerance on the magnitude of the gradient\n",
    "        Outputs:\n",
    "         - a k x d dimensional array of weights w that minimizes f\n",
    "        \"\"\"\n",
    "        w = w0\n",
    "        g = grad_f(w)\n",
    "        while np.linalg.norm(g) > eps:  \n",
    "            g = grad_f(w)\n",
    "            w = w - alpha * grad_f(w)\n",
    "            #print(np.linalg.norm(g))\n",
    "        return w\n",
    "    \n",
    "    @staticmethod\n",
    "    def loss(W, X, y):\n",
    "        \"\"\"\n",
    "        Loss for Logistic Regression \n",
    "        Inputs:\n",
    "         - W: a k x d weight matrix where rows are the d weights for kth class\n",
    "         - X: the n x d data matrix (n observations, d features)\n",
    "         - y: the target classes\n",
    "        Outputs:\n",
    "         - a number that should mean something to someone\n",
    "        \"\"\"\n",
    "        one_hot = softmax_regression.one_hot(y)\n",
    "\n",
    "        W = W.reshape((len(np.unique(y)), X.shape[1]))\n",
    "        xw = X @ W.T \n",
    "        lse = softmax_regression.log_sum_exp(xw)\n",
    "        return -np.sum(one_hot * (xw - lse[:,None])) \n",
    "    \n",
    "    @staticmethod\n",
    "    def grad_loss(W, X, y):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "         - W: k x d array of parameters\n",
    "         - X: n x d design matrix\n",
    "         - y: n x 1 target vector\n",
    "        Output:\n",
    "         - a k x d array of partial derivatives\n",
    "        \"\"\"\n",
    "        one_hot = softmax_regression.one_hot(y)\n",
    "\n",
    "        W = W.reshape((len(np.unique(y)), X.shape[1]))\n",
    "\n",
    "        g = X.T @ (one_hot - softmax_regression.softmax(X @ W.T)) \n",
    "        return -g.T #-g.flatten(order='F')\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit method:\n",
    "        Input:\n",
    "         - X: an n x d design matrix\n",
    "         - y: a n x 1 target vector\n",
    "        Ouput:\n",
    "         - a k x d array of optimal weights\n",
    "        \"\"\"\n",
    "        f = lambda w: softmax_regression.loss(w, X, y)\n",
    "        grad_f = lambda w: softmax_regression.grad_loss(w, X, y)\n",
    "        w0 = np.random.rand(len(np.unique(y))*X.shape[1])\n",
    "        w0 = w0.reshape((len(np.unique(y)), X.shape[1]))\n",
    "        self.W = softmax_regression.gradient_descent(f, grad_f, w0)\n",
    "        return None\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Multiclass soft prediction method:\n",
    "        Input:\n",
    "         - X: an n x d design matrix\n",
    "        Output:\n",
    "         - a n x k array of probabilistic predictions\n",
    "           each observation gets k predicted probabilities \n",
    "         \n",
    "         - note: if you want hard predictions, use out.argmax(axis=1)\n",
    "           where out is what this method returns\n",
    "        \"\"\"\n",
    "        return softmax_regression.softmax(X @ self.W.T)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for comparison\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load some dataset to check how it works\n",
    "digits = sklearn.datasets.load_digits()\n",
    "X, Xvalidate, y, yvalidate = train_test_split(digits.data, digits.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bradley/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='saga',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit my implementation and sklearn's version\n",
    "my_clf = softmax_regression()\n",
    "clf = LogisticRegression(multi_class='multinomial', solver='saga', C=1000)\n",
    "my_clf.fit(X,y)\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# my implementations accuracy:\n",
    "train_pred = my_clf.predict(X).argmax(axis=1)\n",
    "val_pred = my_clf.predict(Xvalidate).argmax(axis=1)\n",
    "\n",
    "my_train = np.sum(train_pred == y) / len(y)\n",
    "my_val = np.sum(val_pred == yvalidate) / len(yvalidate)\n",
    "\n",
    "# sklearn's accuracy:\n",
    "sk_train = clf.score(X, y)\n",
    "sk_val = clf.score(Xvalidate, yvalidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My implementation training accuracy: 100.0\n",
      "My implementation validation accuracy: 96.22222222222221\n",
      "\n",
      "Sklearn's training accuracy: 100.0\n",
      "Sklearn's validation accuracy: 97.33333333333334\n"
     ]
    }
   ],
   "source": [
    "print(\"My implementation training accuracy: {}\".format(100*my_train))\n",
    "print(\"My implementation validation accuracy: {}\\n\".format(100*my_val))\n",
    "print(\"Sklearn's training accuracy: {}\".format(100*sk_train))\n",
    "print(\"Sklearn's validation accuracy: {}\".format(100*sk_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So my implementation is not doing as well as sklearn's (which is no surprise). At this stage I do not have an explaination for this but hopefully I can find a reason in sklearn's source code.\n",
    "\n",
    "To do:\n",
    "\n",
    "- Stochastic gradient descent.\n",
    "- Line search for step size descent algorithm(s).\n",
    "- Change of basis functionality for nonlinear boundaries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
